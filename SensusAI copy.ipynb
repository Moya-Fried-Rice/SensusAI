{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0074760-1d1f-4803-821b-dd14ef218b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Read the CSV file (fixed the quotes)\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\abcjv\\\\Downloads\\\\spotify_reviews.csv\", usecols=['content', 'score'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd4f0c-49a4-412e-bfad-dbf03256bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert rating stars to sentiment\n",
    "def stars_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return 'negative'\n",
    "    elif stars == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Ensure 'score' column exists before applying\n",
    "if 'score' in df.columns:\n",
    "    df['sentiment'] = df['score'].apply(stars_to_sentiment)\n",
    "else:\n",
    "    print(\"Error: Column 'score' not found in DataFrame.\")\n",
    "\n",
    "# Correct way to select multiple columns\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b3fb7-5425-4f31-91a1-532e6e780f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    contractions = {\n",
    "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "        \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "        \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"isn't\": \"is not\",\n",
    "        \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\", \"needn't\": \"need not\", \"needn't\": \"need not\", \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"shouldn't\": \"should not\",\n",
    "        \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"what's\": \"what is\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what've\": \"what have\",\n",
    "        \"where's\": \"where is\", \"where've\": \"where have\", \"who's\": \"who is\", \"who'll\": \"who will\",\n",
    "        \"who're\": \"who are\", \"who've\": \"who have\", \"why's\": \"why is\", \"why're\": \"why are\",\n",
    "        \"why've\": \"why have\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Remove punctuation using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Define English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Customize the stopwords list for sentiment analysis (add negations or important words)\n",
    "    sentiment_important_words = {\"not\", \"no\", \"very\", \"good\", \"bad\", \"excellent\", \"love\", \"hate\", \"great\", \"feel\", \"wish\", \"would\", \"should\"}\n",
    "    stop_words = stop_words - sentiment_important_words  # Remove sentiment important words from stopwords\n",
    "\n",
    "    # Function to map POS tag to wordnet format\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to NOUN if unknown\n",
    "\n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Apply lemmatization based on POS tags and filter out stopwords\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags\n",
    "        if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Join the lemmatized tokens back into a single string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Test the function\n",
    "df['processed_content'] = df['content'].apply(preprocess_text)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e65968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Oversampling neutral | Success: 0.00% | Errors: 100.00%: 100%|██████████| 36887/36887 [01:17<00:00, 474.53sample/s]\n",
      "Oversampling positive | Success: 0.00% | Errors: 100.00%: 100%|██████████| 25666/25666 [00:57<00:00, 445.60sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "negative    48906\n",
      "neutral     36887\n",
      "positive    25666\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Translation Results:\n",
      "Success: 0.00% (0 samples)\n",
      "Errors: 100.00% (62553 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from sklearn.utils import resample\n",
    "from deep_translator import GoogleTranslator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import pandas as pd\n",
    "\n",
    "# Global variable to track the number of requests made today\n",
    "requests_today = 0\n",
    "MAX_REQUESTS_PER_DAY = 200000\n",
    "REQUESTS_PER_SECOND = 5\n",
    "SECONDS_PER_REQUEST = 1 / REQUESTS_PER_SECOND\n",
    "\n",
    "# Counters for tracking errors and successes\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "# Function to perform back-translation and preprocessing with error handling\n",
    "def back_translate_batch(texts, src_lang, intermediate_lang1, intermediate_lang2):\n",
    "    global requests_today, success_count, error_count\n",
    "    \n",
    "    if requests_today >= MAX_REQUESTS_PER_DAY:\n",
    "        print(\"Daily request limit reached!\")\n",
    "        error_count += len(texts)  # Count all texts as errors if the limit is reached\n",
    "        return [preprocess_text(text) for text in texts]  # Return original texts if request limit is reached\n",
    "\n",
    "    try:\n",
    "        # Translate to the first intermediate language\n",
    "        translated1 = GoogleTranslator(source=src_lang, target=intermediate_lang1).translate_batch(texts)\n",
    "        \n",
    "        # Translate to the second intermediate language\n",
    "        translated2 = GoogleTranslator(source=intermediate_lang1, target=intermediate_lang2).translate_batch(translated1)\n",
    "        \n",
    "        # Translate back to the source language\n",
    "        back_translated = GoogleTranslator(source=intermediate_lang2, target=src_lang).translate_batch(translated2)\n",
    "        \n",
    "        # Increment request counter\n",
    "        requests_today += 3 * len(texts)  # One request for each translation stage (3 translations per text)\n",
    "        \n",
    "        time.sleep(SECONDS_PER_REQUEST * len(texts))  # Ensure we don't exceed the requests per second limit\n",
    "        \n",
    "        success_count += len(texts)  # Increment success counter\n",
    "        return [preprocess_text(text) for text in back_translated]\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If any translation fails, log the error and return the original texts\n",
    "        # print(f\"Error during translation: {e}\")\n",
    "        error_count += len(texts)  # Increment error counter\n",
    "        return [preprocess_text(text) for text in texts]  # Return the original texts if translation fails\n",
    "\n",
    "# Parallelized function to oversample with back-translation and show progress bar\n",
    "def oversample_with_back_translation(samples, oversample_count, sentiment_label, src_lang='en', intermediate_lang1='es', intermediate_lang2='fr'):\n",
    "    global success_count, error_count\n",
    "    \n",
    "    # Create new samples using back-translation\n",
    "    new_samples = []\n",
    "    \n",
    "    # Wrap the loop with ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        # Wrap the range of oversampling count with tqdm to show progress\n",
    "        with tqdm(total=oversample_count, desc=f'Oversampling {sentiment_label}', unit='sample') as pbar:\n",
    "            # Submit tasks to executor\n",
    "            for _ in range(0, oversample_count, 100):  # Process in batches of 100\n",
    "                batch_size = min(100, oversample_count - _)\n",
    "                random_samples = random.choices(samples['content'].tolist(), k=batch_size)  # Choose random samples\n",
    "                futures.append(executor.submit(back_translate_batch, random_samples, src_lang, intermediate_lang1, intermediate_lang2))\n",
    "            \n",
    "            # Collect the results as they finish and update the progress bar\n",
    "            for future in futures:\n",
    "                translated_texts = future.result()  # This will block until the task is finished\n",
    "                for text in translated_texts:\n",
    "                    new_samples.append({\n",
    "                        'content': text,\n",
    "                        'sentiment': sentiment_label\n",
    "                    })\n",
    "                pbar.update(len(translated_texts))  # Update progress bar after each completed batch\n",
    "                \n",
    "                # Update progress bar description with success and error percentages\n",
    "                total_processed = success_count + error_count\n",
    "                if total_processed > 0:\n",
    "                    success_percentage = (success_count / total_processed) * 100\n",
    "                    error_percentage = (error_count / total_processed) * 100\n",
    "                    pbar.set_description(f\"Oversampling {sentiment_label} | Success: {success_percentage:.2f}% | Errors: {error_percentage:.2f}%\")\n",
    "    \n",
    "    # Convert new samples to a DataFrame\n",
    "    new_samples_df = pd.DataFrame(new_samples)\n",
    "    \n",
    "    return new_samples_df\n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "# Separate the data by sentiment\n",
    "negative_samples = df[df['sentiment'] == 'negative']\n",
    "neutral_samples = df[df['sentiment'] == 'neutral']\n",
    "positive_samples = df[df['sentiment'] == 'positive']\n",
    "\n",
    "# Define target size (matching the negative class size)\n",
    "target_size = len(negative_samples)\n",
    "\n",
    "# Calculate oversample count for each class\n",
    "neutral_oversample_count = max(target_size - len(neutral_samples), 0)\n",
    "positive_oversample_count = max(target_size - len(positive_samples), 0)\n",
    "\n",
    "# Oversample each class with back-translation\n",
    "neutral_oversampled = oversample_with_back_translation(neutral_samples, neutral_oversample_count, 'neutral')\n",
    "positive_oversampled = oversample_with_back_translation(positive_samples, positive_oversample_count, 'positive')\n",
    "\n",
    "# Combine the oversampled classes to create a more balanced dataset\n",
    "balanced_df = pd.concat([negative_samples, neutral_oversampled, positive_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print the class balance\n",
    "print(balanced_df['sentiment'].value_counts())  # Check class balance\n",
    "\n",
    "# Print final success and error counts\n",
    "total_samples = success_count + error_count\n",
    "if total_samples > 0:\n",
    "    success_percentage = (success_count / total_samples) * 100\n",
    "    error_percentage = (error_count / total_samples) * 100\n",
    "    print(f\"\\nFinal Translation Results:\")\n",
    "    print(f\"Success: {success_percentage:.2f}% ({success_count} samples)\")\n",
    "    print(f\"Errors: {error_percentage:.2f}% ({error_count} samples)\")\n",
    "else:\n",
    "    print(\"\\nNo translations were performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Count the number of occurrences for each sentiment category\n",
    "sentiment_count = balanced_df['sentiment'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Set the figure size\n",
    "\n",
    "# Create a pie chart to visualize sentiment distribution\n",
    "plt.pie(sentiment_count, labels=sentiment_count.index, autopct=\"%1.1f%%\", startangle=140)\n",
    "\n",
    "plt.title('Sentiment Distribution')  # Add a title to the chart\n",
    "plt.axis('equal')  # Ensure the pie chart is displayed as a circle\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_df['processed_content'], balanced_df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with bigrams, minimum document frequency of 3, and max document frequency of 90%\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)\n",
    "\n",
    "# Fit the vectorizer on training data and transform it into numerical feature vectors\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_vectorized, y_train)\n",
    "preds = rf.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
